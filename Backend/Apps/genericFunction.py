import openai
import Apps.config

# def LLM(model, messages, temperature=0, top_p=1, max_tokens=4096):
def LLM():
    # response = openai.chat.completions.create(
    #     model=model,
    #     messages=messages,
    #     temperature=temperature
    # )
    # message = response.choices[0].message.content
    # return message
    return "test"